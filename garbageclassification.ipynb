{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **REGex Final Project TEAM 10**","metadata":{}},{"cell_type":"markdown","source":"**Contributors:**\n* Yashraj Bharambe\n* Diptesh Chaudhari\n* Pranav Karanjawane\n* Shrikrushnakumar Sondge\n* Ashutosh Mishra\n\n**Project Description:**\nA computer vision approach to classifying garbage into recycling categories could be an efficient way to process waste. Recycling is already significant work for all countries. Among the work needed for recycling, garbage classification is the most fundamental step to enable cost-efficient recycling. This project aims to take images of a single piece of recycling or garbage and classify it into one out of six classes consisting of glass, paper, metal, plastic, cardboard, and trash.","metadata":{}},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-10T11:05:25.920545Z","iopub.execute_input":"2021-10-10T11:05:25.920974Z","iopub.status.idle":"2021-10-10T11:05:25.929028Z","shell.execute_reply.started":"2021-10-10T11:05:25.920939Z","shell.execute_reply":"2021-10-10T11:05:25.928251Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# **Setting up File locations and Checking sample images**","metadata":{}},{"cell_type":"code","source":"# setting the path and the labels list for classification of targets on the basis in human understandable form\n\ntrain_dir = os.path.join('/kaggle/input/garbage-classification/TRAIN')\nlabels = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:25.930825Z","iopub.execute_input":"2021-10-10T11:05:25.931476Z","iopub.status.idle":"2021-10-10T11:05:25.937373Z","shell.execute_reply.started":"2021-10-10T11:05:25.931374Z","shell.execute_reply":"2021-10-10T11:05:25.936498Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# checking the size of data available to us for training out model\n\nfor label in labels:\n    directory = os.path.join(train_dir, label)\n    print(\"Images of label \\\"\" + label + \"\\\":\\t\", len(os.listdir(directory)))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:25.940348Z","iopub.execute_input":"2021-10-10T11:05:25.940584Z","iopub.status.idle":"2021-10-10T11:05:25.955596Z","shell.execute_reply.started":"2021-10-10T11:05:25.940551Z","shell.execute_reply":"2021-10-10T11:05:25.954789Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# plotting images of different review for understanding the dataset\n\nplt.figure(figsize=(30,14))\n\nfor i in range(6):\n    directory = os.path.join(train_dir, labels[i])\n    for j in range(10):\n        path = os.path.join(directory, os.listdir(directory)[j])\n        img = mpimg.imread(path)\n        \n        plt.subplot(6, 10, i*10 + j + 1)\n        plt.imshow(img)\n        \n        if j == 0:\n            plt.ylabel(labels[i], fontsize=20)\n        \nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:25.958699Z","iopub.execute_input":"2021-10-10T11:05:25.958898Z","iopub.status.idle":"2021-10-10T11:05:30.686479Z","shell.execute_reply.started":"2021-10-10T11:05:25.958877Z","shell.execute_reply":"2021-10-10T11:05:30.685761Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# checking size of individual image\n\ndirectory = os.path.join(train_dir, 'cardboard')\npath = os.path.join(directory, os.listdir(directory)[0])\nimage = mpimg.imread(path)\nimage.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:30.687593Z","iopub.execute_input":"2021-10-10T11:05:30.688378Z","iopub.status.idle":"2021-10-10T11:05:30.702233Z","shell.execute_reply.started":"2021-10-10T11:05:30.688339Z","shell.execute_reply":"2021-10-10T11:05:30.701666Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Deep Learning**","metadata":{}},{"cell_type":"code","source":"# creating the model\n\nmodel = tf.keras.models.load_model(\"../input/lastone/lastone.h5\")\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:30.703236Z","iopub.execute_input":"2021-10-10T11:05:30.703714Z","iopub.status.idle":"2021-10-10T11:05:31.770913Z","shell.execute_reply.started":"2021-10-10T11:05:30.703679Z","shell.execute_reply":"2021-10-10T11:05:31.770152Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(lr = 0.0002), metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:31.772815Z","iopub.execute_input":"2021-10-10T11:05:31.773078Z","iopub.status.idle":"2021-10-10T11:05:31.791174Z","shell.execute_reply.started":"2021-10-10T11:05:31.773043Z","shell.execute_reply":"2021-10-10T11:05:31.790504Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# creating generators for combining data and increasing the gainable insights by slightly modifying the images in the dataset\n\ntrain_datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,\n                                   rotation_range=15,zoom_range=0.1,\n                                   width_shift_range=0.15,height_shift_range=0.15,\n                                   shear_range=0.1,\n                                   fill_mode=\"nearest\",\n                                   rescale=1./255., \n                                   validation_split=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size=(150, 200), batch_size=32, class_mode = 'binary', subset='training')\nvalidation_generator = train_datagen.flow_from_directory(train_dir, target_size=(150, 200), batch_size=32, shuffle = True, class_mode = 'binary', subset='validation')","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:31.792178Z","iopub.execute_input":"2021-10-10T11:05:31.792441Z","iopub.status.idle":"2021-10-10T11:05:32.008058Z","shell.execute_reply.started":"2021-10-10T11:05:31.792393Z","shell.execute_reply":"2021-10-10T11:05:32.007351Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# creating the callback function so that it can be used to end the training in case reached a good accuracy rate (above 90%)\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.95):\n            print(\"\\nReached 95% accuracy so cancelling training!\")\n            self.model.stop_training = True\n\ncallbacks = myCallback()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:32.010928Z","iopub.execute_input":"2021-10-10T11:05:32.011131Z","iopub.status.idle":"2021-10-10T11:05:32.018625Z","shell.execute_reply.started":"2021-10-10T11:05:32.011107Z","shell.execute_reply":"2021-10-10T11:05:32.017869Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, epochs=30, verbose=1, validation_data=validation_generator, callbacks=[callbacks])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:05:32.022205Z","iopub.execute_input":"2021-10-10T11:05:32.022437Z","iopub.status.idle":"2021-10-10T11:15:01.985325Z","shell.execute_reply.started":"2021-10-10T11:05:32.022392Z","shell.execute_reply":"2021-10-10T11:15:01.984621Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.save(\"lastone.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:15:01.986777Z","iopub.execute_input":"2021-10-10T11:15:01.987145Z","iopub.status.idle":"2021-10-10T11:15:02.176995Z","shell.execute_reply.started":"2021-10-10T11:15:01.987105Z","shell.execute_reply":"2021-10-10T11:15:02.176136Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n#cat = int(input('Enter any category by index: '))\n#ind = int(input('Enter any index to test: '))\nfor cat in range(6):\n    directory = os.path.join(train_dir, labels[cat % 6])\n    for ind in range(10, 15):\n        path = os.path.join(directory, os.listdir(directory)[ind])\n        img = Image.open(path)\n        img = img.resize((150, 200))\n        display(img)\n        x = keras.preprocessing.image.img_to_array(img)\n        x = x/255\n        x = np.expand_dims(x, axis=0)\n\n        images = np.vstack([x])\n        classes = model.predict(images)\n        pred = labels[np.argmax(classes)]\n    \n        print(\"Actual: \", labels[cat % 6], \" Prediction: \", pred)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:15:02.179468Z","iopub.execute_input":"2021-10-10T11:15:02.179993Z","iopub.status.idle":"2021-10-10T11:15:03.966375Z","shell.execute_reply.started":"2021-10-10T11:15:02.179954Z","shell.execute_reply":"2021-10-10T11:15:03.965627Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}